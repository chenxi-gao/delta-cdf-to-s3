{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from new_air.outbound.commons.cdc_loader import CDCLoader\n",
    "from new_air.outbound.commons.load_status_delta import (\n",
    "    PipelineHelperOutbound,\n",
    "    PipelineLayerOutbound,\n",
    ")\n",
    "from new_air.outbound.commons.destination import DestinationType\n",
    "from new_air.outbound.destinations.s3_helper.location_finder import LocationFinder\n",
    "from new_air.outbound.destinations.s3 import S3Destination\n",
    "\n",
    "# create logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ensure root logger is configured correctly\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "for handler in root_logger.handlers:\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read required parameters from Databricks job widgets\n",
    "try:\n",
    "    CATALOG_NAME = dbutils.widgets.get(\"catalog\")\n",
    "    SCHEMA_NAME = dbutils.widgets.get(\"schema\")\n",
    "    TABLE_NAME = dbutils.widgets.get(\"table_name\")\n",
    "\n",
    "    S3_BUCKET_KEYWORD = dbutils.widgets.get(\"s3_bucket_keyword\")\n",
    "    S3_PREFIX = dbutils.widgets.get(\"s3_prefix\")\n",
    "    S3_FILENAME_GENERATOR = dbutils.widgets.get(\"s3_filename_generator\")\n",
    "    S3_OUTPUT_FORMAT = dbutils.widgets.get(\"s3_output_format\")\n",
    "\n",
    "    AUTO_FIND_S3_BY_KEYWORD = dbutils.widgets.get(\"auto_find_s3_by_keyword\")\n",
    "    S3_BUCKET_FULL_URL = dbutils.widgets.get(\"s3_bucket_full_url\")\n",
    "except Exception:\n",
    "    raise ValueError(\"Missing required job parameters. Ensure the job passes catalog, schema, table_name, s3_bucket_keyword, s3_prefix, s3_filename_generator, s3_output_format, auto_find_s3_by_keyword, s3_bucket_full_url\")\n",
    "\n",
    "# Basic validation\n",
    "for _key, _val in {\n",
    "    \"catalog\": CATALOG_NAME,\n",
    "    \"schema\": SCHEMA_NAME,\n",
    "    \"table_name\": TABLE_NAME,\n",
    "    \"s3_filename_generator\": S3_FILENAME_GENERATOR,\n",
    "    \"s3_output_format\": S3_OUTPUT_FORMAT,\n",
    "}.items():\n",
    "    if _val is None or str(_val).strip() == \"\":\n",
    "        raise ValueError(f\"Parameter '{_key}' must be provided and non-empty\")\n",
    "\n",
    "# Conditional validation\n",
    "_auto_flag = str(AUTO_FIND_S3_BY_KEYWORD or \"\").strip().lower()\n",
    "if _auto_flag in (\"true\", \"1\", \"yes\"):\n",
    "    if S3_BUCKET_KEYWORD is None or str(S3_BUCKET_KEYWORD).strip() == \"\":\n",
    "        raise ValueError(\"'s3_bucket_keyword' must be provided when 'auto_find_s3_by_keyword' is true\")\n",
    "else:\n",
    "    if S3_BUCKET_FULL_URL is None or str(S3_BUCKET_FULL_URL).strip() == \"\":\n",
    "        raise ValueError(\"'s3_bucket_full_url' must be provided when 'auto_find_s3_by_keyword' is false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark and environment context\n",
    "logging.info(\"Initializing Spark session and environment context\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "os.environ[\"CATALOG_NAME\"] = CATALOG_NAME\n",
    "\n",
    "# Build names\n",
    "qualified_source_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "source_table_only = qualified_source_table.split(\".\")[-1]\n",
    "\n",
    "# Prepare load status helper by layer (schema aligned) - version based\n",
    "layer = PipelineLayerOutbound[SCHEMA_NAME]\n",
    "helper = PipelineHelperOutbound(layer=layer)\n",
    "helper.create_version_based_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read last processed version and last run time from status table\n",
    "status_df = helper.get_load_status_data_version_based()\n",
    "\n",
    "_filtered = (\n",
    "    status_df.where(F.col(\"table_name\") == F.lit(source_table_only))\n",
    "    .where(F.col(\"destination_type\") == F.lit(DestinationType.S3.value))\n",
    "    .select(\"last_processed_version\", \"last_run_time\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "last_processed_version_saved = None\n",
    "last_run_date = None\n",
    "if _filtered:\n",
    "    _lpv = _filtered[0][\"last_processed_version\"]\n",
    "    last_processed_version_saved = int(_lpv) if _lpv is not None else None\n",
    "    _lrt = _filtered[0][\"last_run_time\"]\n",
    "    if _lrt is not None:\n",
    "        last_run_date = _lrt.date()\n",
    "\n",
    "this_run_time = datetime.utcnow()\n",
    "this_run_date = this_run_time.date()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ea6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute version bounds and derive last_run_date if necessary\n",
    "PROCESSING_REQUIRED = True\n",
    "min_version = None\n",
    "max_version = None\n",
    "\n",
    "if last_processed_version_saved is None:\n",
    "    history_df = spark.sql(f\"DESCRIBE HISTORY {qualified_source_table}\")\n",
    "    if not history_df.head(1):\n",
    "        logger.info(\"No history found for source table; nothing to process.\")\n",
    "        PROCESSING_REQUIRED = False\n",
    "    else:\n",
    "        agg_row = history_df.agg(\n",
    "            F.min(F.col(\"version\")).alias(\"min_ver\"),\n",
    "            F.max(F.col(\"version\")).alias(\"max_ver\"),\n",
    "        ).collect()[0]\n",
    "        min_version = int(agg_row[\"min_ver\"]) if agg_row[\"min_ver\"] is not None else None\n",
    "        max_version = int(agg_row[\"max_ver\"]) if agg_row[\"max_ver\"] is not None else None\n",
    "        if min_version is None or max_version is None:\n",
    "            logger.info(\"No history found for source table; nothing to process.\")\n",
    "            PROCESSING_REQUIRED = False\n",
    "        if PROCESSING_REQUIRED and last_run_date is None:\n",
    "            window_df = history_df.where(\n",
    "                (F.col(\"version\") >= F.lit(int(min_version))) & (F.col(\"version\") <= F.lit(int(max_version)))\n",
    "            )\n",
    "            if window_df.head(1):\n",
    "                ts_row = window_df.agg(\n",
    "                    F.min(F.col(\"timestamp\")).alias(\"min_ts\"),\n",
    "                    F.max(F.col(\"timestamp\")).alias(\"max_ts\"),\n",
    "                ).collect()[0]\n",
    "                min_ts = ts_row[\"min_ts\"]\n",
    "                last_run_date = (min_ts or datetime.utcnow()).date()\n",
    "else:\n",
    "    history_df = spark.sql(f\"DESCRIBE HISTORY {qualified_source_table}\")\n",
    "    if not history_df.head(1):\n",
    "        logger.info(\"No history found for source table; nothing to process.\")\n",
    "        PROCESSING_REQUIRED = False\n",
    "    else:\n",
    "        agg_row = history_df.agg(F.max(F.col(\"version\")).alias(\"max_ver\")).collect()[0]\n",
    "        current_latest_version = agg_row[\"max_ver\"]\n",
    "        if current_latest_version is None or int(current_latest_version) <= int(last_processed_version_saved):\n",
    "            logger.info(\"No new versions detected; nothing to process.\")\n",
    "            PROCESSING_REQUIRED = False\n",
    "        else:\n",
    "            min_version = int(last_processed_version_saved) + 1\n",
    "            max_version = int(current_latest_version)\n",
    "\n",
    "logging.info(f\"PROCESSING_REQUIRED={PROCESSING_REQUIRED}, min_version={min_version}, max_version={max_version}, last_run_date={last_run_date}, this_run_date={this_run_date}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDC extraction by version\n",
    "if not PROCESSING_REQUIRED:\n",
    "    logging.info(\"Skipping CDC extraction as no processing is required.\")\n",
    "else:\n",
    "    cdc_loader = CDCLoader(spark)\n",
    "    try:\n",
    "        staging_df = cdc_loader.process_table_changes_by_version(\n",
    "            table_name=qualified_source_table,\n",
    "            min_version=int(min_version),\n",
    "            max_version=int(max_version),\n",
    "        )\n",
    "        logging.info(\"CDC extraction completed.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"CDC extraction by version failed: {e}\")\n",
    "        PROCESSING_REQUIRED = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve S3 target location\n",
    "if not PROCESSING_REQUIRED:\n",
    "    logging.info(\"Skipping S3 location resolution as no processing is required.\")\n",
    "else:\n",
    "    try:\n",
    "        environment = None\n",
    "        _auto_flag = str(AUTO_FIND_S3_BY_KEYWORD or \"\").strip().lower()\n",
    "        if _auto_flag in (\"true\", \"1\", \"yes\"):\n",
    "            loc_finder = LocationFinder(dbutils_instance=dbutils, spark_instance=spark)\n",
    "            write_location = loc_finder.get_write_location_for_keyword(S3_BUCKET_KEYWORD)\n",
    "            s3_bucket = write_location[\"bucket\"]\n",
    "            environment = write_location.get(\"environment\") or loc_finder.get_environment()\n",
    "            s3_url = f\"s3://{s3_bucket}/{S3_PREFIX}\" if S3_PREFIX else f\"s3://{s3_bucket}\"\n",
    "        else:\n",
    "            s3_url = S3_BUCKET_FULL_URL.rstrip(\"/\")\n",
    "            try:\n",
    "                loc_finder = LocationFinder(dbutils_instance=dbutils, spark_instance=spark)\n",
    "                environment = loc_finder.get_environment()\n",
    "            except Exception:\n",
    "                environment = None\n",
    "        logging.info(f\"Resolved S3 URL: {s3_url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to resolve S3 location: {e}\")\n",
    "        PROCESSING_REQUIRED = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to S3 and update version-based status\n",
    "if not PROCESSING_REQUIRED:\n",
    "    logging.info(\"Skipping write and status update as no processing is required.\")\n",
    "else:\n",
    "    s3_dest = S3Destination(\n",
    "        spark=spark,\n",
    "        s3_options={\n",
    "            \"url\": s3_url,\n",
    "            \"format\": S3_OUTPUT_FORMAT,\n",
    "            \"filename_generator_type\": S3_FILENAME_GENERATOR,\n",
    "            \"filename_generator_params\": ({\"environment\": environment} if environment else {}),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    base_params = {\n",
    "        \"start_date\": last_run_date,\n",
    "        \"end_date\": this_run_date,\n",
    "    }\n",
    "\n",
    "    write_ok = False\n",
    "    try:\n",
    "        write_ok = s3_dest.write_data(\n",
    "            df=staging_df,\n",
    "            output_filename=source_table_only,\n",
    "            base_params=base_params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Write to S3 via destination failed: {e}\")\n",
    "        write_ok = False\n",
    "\n",
    "    if write_ok:\n",
    "        try:\n",
    "            last_processed_version = s3_dest.compute_last_processed_version(staging_df)\n",
    "            s3_dest.update_version_based_status(\n",
    "                helper,\n",
    "                table_name=source_table_only,\n",
    "                last_processed_version=last_processed_version,\n",
    "                last_run_time=this_run_time,\n",
    "            )\n",
    "            logger.info(\"Version-based load status updated successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to update version-based load status: {e}\")\n",
    "    else:\n",
    "        logger.error(\"Write to S3 failed; version-based load status not updated.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abacus-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
