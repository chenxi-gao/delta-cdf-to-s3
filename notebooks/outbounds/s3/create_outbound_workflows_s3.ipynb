{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc06831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Modification Settings\n",
    "config_file_path = f\"configurations/outbound-s3-export.yaml\"\n",
    "workflow_name_prefix = \"outbound_s3_export\"\n",
    "policy_id = \"001014E447543471\"\n",
    "aws_profile_arn = \"arn:aws:iam::951193945349:instance-profile/abacus-compute-pipeline-dev\"\n",
    "# notebook_base_path = \"/Repos/chenxi.gao@abacusinsights.com/ng-pipelines-example/\" #prod\n",
    "notebook_base_path = \"/Workspace/Users/chenxi.gao@abacusinsights.com/xport-1540-v2/\" #dev\n",
    "timeout_seconds = 1800  # 30 minutes\n",
    "min_retry_interval_millis = 5000  # 5 seconds\n",
    "retry_on_timeout_default = False\n",
    "spark_version = \"14.3.x-scala2.12\"\n",
    "node_type_id = \"i4i.xlarge\"\n",
    "num_workers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "with open(config_file_path, 'r') as f:\n",
    "    raw_cfg: Dict[str, Any] = yaml.safe_load(f)\n",
    "\n",
    "settings = raw_cfg.get('settings', {})\n",
    "catalog = settings.get('catalog')\n",
    "schema = settings.get('schema')\n",
    "destination_type = settings.get('destination_type', 's3')\n",
    "groups = settings.get('groups', [])\n",
    "\n",
    "if not catalog or not schema:\n",
    "    raise ValueError(\"'catalog' and 'schema' must be defined in settings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01644554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from databricks_cli.sdk.api_client import ApiClient\n",
    "from databricks_cli.jobs.api import JobsApi\n",
    "\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "api_token = ctx.apiToken().get()\n",
    "api_url = ctx.apiUrl().get()\n",
    "\n",
    "api_client = ApiClient(host=api_url, token=api_token)\n",
    "\n",
    "\n",
    "def create_or_reset_job(job_settings: Dict[str, Any], jobs_reset: bool = True):\n",
    "    job_name = job_settings['name']\n",
    "    jobs_api = JobsApi(api_client)\n",
    "    jobs = jobs_api.list_jobs(name=job_name)\n",
    "    matching_jobs = (j for j in jobs.get(\"jobs\", []) if j[\"settings\"][\"name\"] == job_name)\n",
    "    desired_job = next(matching_jobs, None)\n",
    "    if desired_job is not None:\n",
    "        logging.info(f\"{job_name} already exists. Updating/Resetting...\")\n",
    "        current_job_id = desired_job[\"job_id\"]\n",
    "        new_job_settings = {\"job_id\": current_job_id, \"new_settings\": job_settings}\n",
    "        endpoint = \"reset\" if jobs_reset else \"update\"\n",
    "        response = requests.post(f\"{api_url}/api/2.1/jobs/{endpoint}\", json=new_job_settings,\n",
    "                                 headers={\"Authorization\": f\"Bearer {api_token}\"})\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Job successfully updated: {job_name}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Job update failed: {response.json().get('message')}\")\n",
    "    else:\n",
    "        response = requests.post(f\"{api_url}/api/2.1/jobs/create\", json=job_settings,\n",
    "                                 headers={\"Authorization\": f\"Bearer {api_token}\"})\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Job successfully created: {job_name}; id={response.json().get('job_id')}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Job creation failed: {response.json().get('message')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af07d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a single job per group with one task per table\n",
    "job_cluster_single = [{\n",
    "    \"job_cluster_key\": \"s3_export_cluster\",\n",
    "    \"new_cluster\": {\n",
    "        \"spark_version\": spark_version,\n",
    "        \"aws_attributes\": {\"instance_profile_arn\": aws_profile_arn},\n",
    "        \"node_type_id\": node_type_id,\n",
    "        \"driver_node_type_id\": node_type_id,\n",
    "        \"policy_id\": policy_id,\n",
    "        \"num_workers\": num_workers,\n",
    "    },\n",
    "}]\n",
    "\n",
    "jobs_payloads = []\n",
    "\n",
    "for group in groups:\n",
    "    group_name = group.get('group_name', 'default')\n",
    "    alert_on_failure = str(group.get('alert_on_failure', 'false')).lower() == 'true'\n",
    "    alert_emails = group.get('alert_emails', []) if alert_on_failure else []\n",
    "    max_retries = int(group.get('max_retries', 0) or 0)\n",
    "    schedule = group.get('schedule')\n",
    "\n",
    "    tables = group.get('tables', [])\n",
    "    if not isinstance(tables, list) or len(tables) == 0:\n",
    "        logging.warning(f\"Group {group_name} has no tables. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    tasks = []\n",
    "    for idx, tbl in enumerate(tables, start=1):\n",
    "        base_parameters = {\n",
    "            \"catalog\": catalog,\n",
    "            \"schema\": schema,\n",
    "            \"table_name\": tbl.get('source_table'),\n",
    "            \"s3_bucket_keyword\": tbl.get('s3_bucket_keyword', 'data-lake'),\n",
    "            \"s3_prefix\": tbl.get('s3_prefix', ''),\n",
    "            \"s3_filename_generator\": tbl.get('s3_filename_generator', 'default'),\n",
    "            \"s3_output_format\": tbl.get('s3_output_format', 'parquet'),\n",
    "            \"auto_find_s3_by_keyword\": tbl.get('auto_find_s3_by_keyword', 'true'),\n",
    "            \"s3_bucket_full_url\": tbl.get('s3_bucket_full_url', ''),\n",
    "        }\n",
    "        if not base_parameters[\"table_name\"]:\n",
    "            raise ValueError(\"Each table entry must include 'source_table'\")\n",
    "\n",
    "        task = {\n",
    "            \"task_key\": f\"run_pipeline_{idx}\",\n",
    "            \"source\": \"WORKSPACE\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{notebook_base_path}notebooks/outbounds/s3/run_pipeline\",\n",
    "                \"base_parameters\": base_parameters,\n",
    "            },\n",
    "            \"job_cluster_key\": \"s3_export_cluster\",\n",
    "            \"retry_on_timeout\": (max_retries > 0) or retry_on_timeout_default,\n",
    "            \"timeout_seconds\": timeout_seconds,\n",
    "            \"max_retries\": max_retries,\n",
    "            \"min_retry_interval_millis\": min_retry_interval_millis,\n",
    "        }\n",
    "        tasks.append(task)\n",
    "\n",
    "    scheduler_dict = None\n",
    "    if schedule:\n",
    "        scheduler_dict = {\"quartz_cron_expression\": f\"{schedule}\", \"timezone_id\": \"US/Eastern\"}\n",
    "\n",
    "    job_payload = {\n",
    "        \"settings\": {\n",
    "            \"name\": f\"{workflow_name_prefix}_{group_name}\",\n",
    "            \"max_concurrent_runs\": 1,\n",
    "            \"timeout_seconds\": timeout_seconds,\n",
    "            \"email_notifications\": {\n",
    "                \"no_alert_for_skipped_runs\": True,\n",
    "                \"on_failure\": alert_emails,\n",
    "            },\n",
    "            \"notification_settings\": {\n",
    "                \"no_alert_for_skipped_runs\": True,\n",
    "                \"no_alert_for_canceled_runs\": True,\n",
    "            },\n",
    "            \"webhook_notifications\": {},\n",
    "            \"tasks\": tasks,\n",
    "            \"job_clusters\": job_cluster_single,\n",
    "            \"format\": \"MULTI_TASK\",\n",
    "            **({\"schedule\": scheduler_dict} if scheduler_dict else {}),\n",
    "        }\n",
    "    }\n",
    "    jobs_payloads.append(job_payload)\n",
    "\n",
    "for job in jobs_payloads:\n",
    "    create_or_reset_job(job[\"settings\"], jobs_reset=True)\n",
    "    logging.info(\"Job created/updated successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
