{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81edc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from new_air.outbound.commons.cdc_loader import CDCLoader\n",
    "from new_air.outbound.commons.load_status_delta import (\n",
    "    PipelineHelperOutbound,\n",
    "    PipelineLayerOutbound,\n",
    ")\n",
    "from new_air.outbound.commons.destination import DestinationType\n",
    "from new_air.outbound.destinations.s3_helper.location_finder import LocationFinder\n",
    "from new_air.outbound.destinations.s3 import S3Destination\n",
    "\n",
    "# flake8: noqa: F821\n",
    "\n",
    "# create logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ensure root logger is configured correctly\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "for handler in root_logger.handlers:\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "# ----------------------------\n",
    "# Required Parameters\n",
    "# ----------------------------\n",
    "# Table Configuration\n",
    "CATALOG_NAME = \"abacus_dev_catalog\"\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "TABLE_NAME = \"test_table_for_integrated_framework\"\n",
    "\n",
    "# S3 Configuration\n",
    "S3_BUCKET_KEYWORD = \"data-lake\"\n",
    "S3_PREFIX = \"cdf-process-test/oynx-reports\"\n",
    "S3_FILENAME_GENERATOR = \"oynx_report\"\n",
    "S3_OUTPUT_FORMAT = \"parquet\"\n",
    "\n",
    "\n",
    "def _qualified_table_name(catalog: str, schema: str, table: str) -> str:\n",
    "    return f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "\n",
    "def _table_name_only(full_table: str) -> str:\n",
    "    return full_table.split(\".\")[-1]\n",
    "\n",
    "\n",
    "def _get_version_bounds(\n",
    "    spark: SparkSession, qualified_table: str\n",
    ") -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"Return (min_version, max_version) from DESCRIBE HISTORY.\"\"\"\n",
    "    history_df: DataFrame = spark.sql(f\"DESCRIBE HISTORY {qualified_table}\")\n",
    "    if history_df.head(1):\n",
    "        row = history_df.agg(\n",
    "            F.min(F.col(\"version\")).alias(\"min_ver\"),\n",
    "            F.max(F.col(\"version\")).alias(\"max_ver\"),\n",
    "        ).collect()[0]\n",
    "        min_ver = row[\"min_ver\"]\n",
    "        max_ver = row[\"max_ver\"]\n",
    "        return (int(min_ver) if min_ver is not None else None, int(max_ver) if max_ver is not None else None)\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _get_ts_bounds_for_versions(\n",
    "    spark: SparkSession, qualified_table: str, min_version: int, max_version: int\n",
    ") -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "    \"\"\"Return (min_timestamp, max_timestamp) for commits within [min_version, max_version].\"\"\"\n",
    "    history_df: DataFrame = spark.sql(f\"DESCRIBE HISTORY {qualified_table}\")\n",
    "    window_df = history_df.where(\n",
    "        (F.col(\"version\") >= F.lit(int(min_version))) & (F.col(\"version\") <= F.lit(int(max_version)))\n",
    "    )\n",
    "    if window_df.head(1):\n",
    "        row = window_df.agg(\n",
    "            F.min(F.col(\"timestamp\")).alias(\"min_ts\"),\n",
    "            F.max(F.col(\"timestamp\")).alias(\"max_ts\"),\n",
    "        ).collect()[0]\n",
    "        return row[\"min_ts\"], row[\"max_ts\"]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _get_last_processed_version(\n",
    "    helper: PipelineHelperOutbound, source_table_name_only: str\n",
    ") -> Optional[int]:\n",
    "    \"\"\"Fetch last_processed_version for this table and S3 destination from version-based status table.\"\"\"\n",
    "    status_df = helper.get_load_status_data_version_based()\n",
    "    filtered = (\n",
    "        status_df.where(F.col(\"table_name\") == F.lit(source_table_name_only))\n",
    "        .where(F.col(\"destination_type\") == F.lit(DestinationType.S3.value))\n",
    "        .select(\"last_processed_version\")\n",
    "        .limit(1)\n",
    "        .collect()\n",
    "    )\n",
    "    if filtered:\n",
    "        last_processed_version = filtered[0][\"last_processed_version\"]\n",
    "        return int(last_processed_version) if last_processed_version is not None else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_last_run_time(\n",
    "    helper: PipelineHelperOutbound, source_table_name_only: str\n",
    ") -> Optional[date]:\n",
    "    \"\"\"Fetch last_run_time from version-based status table and return as date (yyyy-mm-dd).\n",
    "    If no record exists, return None (caller will compute from table history).\n",
    "    \"\"\"\n",
    "    status_df = helper.get_load_status_data_version_based()\n",
    "    filtered = (\n",
    "        status_df.where(F.col(\"table_name\") == F.lit(source_table_name_only))\n",
    "        .where(F.col(\"destination_type\") == F.lit(DestinationType.S3.value))\n",
    "        .select(\"last_run_time\")\n",
    "        .limit(1)\n",
    "        .collect()\n",
    "    )\n",
    "    if filtered and filtered[0][\"last_run_time\"] is not None:\n",
    "        ts: datetime = filtered[0][\"last_run_time\"]\n",
    "        return ts.date()\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_workflow():\n",
    "    logging.info(f\"Running workflow\")\n",
    "\n",
    "    # Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Propagate catalog for load status helper\n",
    "    os.environ[\"CATALOG_NAME\"] = CATALOG_NAME\n",
    "\n",
    "    # Build names\n",
    "    qualified_source_table = _qualified_table_name(CATALOG_NAME, SCHEMA_NAME, TABLE_NAME)\n",
    "    source_table_only = _table_name_only(qualified_source_table)\n",
    "\n",
    "    # Prepare load status helper by layer (schema aligned) - version based\n",
    "    layer = PipelineLayerOutbound[SCHEMA_NAME]\n",
    "    helper = PipelineHelperOutbound(layer=layer)\n",
    "    helper.create_version_based_table()\n",
    "\n",
    "    # Compute version bounds and last_run_time/this_run_time\n",
    "    last_processed_version_saved = _get_last_processed_version(helper, source_table_only)\n",
    "    min_version: Optional[int]\n",
    "    max_version: Optional[int]\n",
    "    this_run_time: datetime = datetime.utcnow()\n",
    "    last_run_date: Optional[date] = _get_last_run_time(helper, source_table_only)\n",
    "    this_run_date: date = this_run_time.date()\n",
    "\n",
    "    if last_processed_version_saved is None:\n",
    "        min_version, max_version = _get_version_bounds(spark, qualified_source_table)\n",
    "        if min_version is None or max_version is None:\n",
    "            logger.info(\"No history found for source table; nothing to process.\")\n",
    "            return\n",
    "        # If no last_run_time recorded, use earliest commit timestamp from history as last_run_date\n",
    "        if last_run_date is None:\n",
    "            min_ts, _ = _get_ts_bounds_for_versions(spark, qualified_source_table, int(min_version), int(max_version))\n",
    "            last_run_date = (min_ts or datetime.utcnow()).date()\n",
    "    else:\n",
    "        # Use last processed + 1 as lower bound; upper bound is current latest commit version\n",
    "        _, current_latest_version = _get_version_bounds(spark, qualified_source_table)\n",
    "        if current_latest_version is None or int(current_latest_version) <= int(last_processed_version_saved):\n",
    "            logger.info(\"No new versions detected; nothing to process.\")\n",
    "            return\n",
    "        min_version = int(last_processed_version_saved) + 1\n",
    "        max_version = int(current_latest_version)\n",
    "\n",
    "\n",
    "    # Prepare CDC loader (version-based)\n",
    "    cdc_loader = CDCLoader(spark)\n",
    "    try:\n",
    "        staging_df: DataFrame = cdc_loader.process_table_changes_by_version(\n",
    "            table_name=qualified_source_table,\n",
    "            min_version=int(min_version),\n",
    "            max_version=int(max_version),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"CDC extraction by version failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Resolve S3 bucket and environment via LocationFinder (Databricks-only)\n",
    "    try:\n",
    "        loc_finder = LocationFinder(dbutils_instance=dbutils, spark_instance=spark)  # noqa: F821\n",
    "        write_location = loc_finder.get_write_location_for_keyword(S3_BUCKET_KEYWORD)\n",
    "        s3_bucket = write_location[\"bucket\"]\n",
    "        environment = write_location.get(\"environment\") or loc_finder.get_environment()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to resolve S3 location: {e}\")\n",
    "        return\n",
    "\n",
    "    # Instantiate S3Destination to handle filename generation and writes\n",
    "    s3_url = f\"s3://{s3_bucket}/{S3_PREFIX}\" if S3_PREFIX else f\"s3://{s3_bucket}\"\n",
    "    s3_dest = S3Destination(\n",
    "        spark=spark,\n",
    "        s3_options={\n",
    "            \"url\": s3_url,\n",
    "            \"format\": S3_OUTPUT_FORMAT,\n",
    "            \"filename_generator_type\": S3_FILENAME_GENERATOR,\n",
    "            \"filename_generator_params\": {\"environment\": environment},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Filename uses last_run_date (start) and this_run_date (end)\n",
    "    base_params = {\n",
    "        \"start_date\": last_run_date,\n",
    "        \"end_date\": this_run_date,\n",
    "    }\n",
    "    write_ok = False\n",
    "    try:\n",
    "        write_ok = s3_dest.write_data(\n",
    "            df=staging_df,\n",
    "            output_filename=source_table_only,\n",
    "            base_params=base_params\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Write to S3 via destination failed: {e}\")\n",
    "        write_ok = False\n",
    "\n",
    "    # Update version-based status only on success\n",
    "    if write_ok:\n",
    "        try:\n",
    "            last_processed_version = s3_dest.compute_last_processed_version(staging_df)\n",
    "            s3_dest.update_version_based_status(\n",
    "                helper,\n",
    "                table_name=source_table_only,\n",
    "                last_processed_version=last_processed_version,\n",
    "                last_run_time=this_run_time,\n",
    "            )\n",
    "            logger.info(\"Version-based load status updated successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to update version-based load status: {e}\")\n",
    "    else:\n",
    "        logger.error(\"Write to S3 failed; version-based load status not updated.\")\n",
    "\n",
    "\n",
    "run_workflow()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
